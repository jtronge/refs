@INPROCEEDINGS{Smith2016,
  author={Smith, Shaden and Karypis, George},
  booktitle={2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={A Medium-Grained Algorithm for Sparse Tensor Factorization}, 
  year={2016},
  volume={},
  number={},
  pages={902-911},
  keywords={Tensile stress;Matrix decomposition;Sparse matrices;Distributed databases;Memory management;Parallel processing;Scalability;Sparse tensor;distributed;PARAFAC;CPD;parallel;medium-grained},
  doi={10.1109/IPDPS.2016.113}}

@INPROCEEDINGS{Smith2015,
  author={Smith, Shaden and Ravindran, Niranjay and Sidiropoulos, Nicholas D. and Karypis, George},
  booktitle={2015 IEEE International Parallel and Distributed Processing Symposium},
  title={SPLATT: Efficient and Parallel Sparse Tensor-Matrix Multiplication},
  year={2015},
  volume={},
  number={},
  pages={61-70},
  keywords={Tensile stress;Sparse matrices;Data structures;Memory management;Context;Algorithm design and analysis;Parallel processing;Sparse tensors;PARAFAC;CANDECOMP;CPD;parallel},
  doi={10.1109/IPDPS.2015.27}}

@inproceedings{Soh2024,
author = {Soh, Yongseok and Kannan, Ramakrishnan and Sao, Piyush and Choi, Jee},
title = {Accelerated Constrained Sparse Tensor Factorization on Massively Parallel Architectures},
year = {2024},
isbn = {9798400717932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673038.3673128},
doi = {10.1145/3673038.3673128},
abstract = {This study presents the first constrained sparse tensor factorization (cSTF) framework that optimizes and fully offloads computation to massively parallel GPU architectures, and the first performance characterization of cSTF on GPU architectures. In contrast to prior work on tensor factorization, where the matricized tensor times Khatri-Rao product (MTTKRP) is the primary performance bottleneck, our systematic analysis of the cSTF algorithm on GPUs reveals that adding constraints creates an additional bottleneck in the update operation for many real-world sparse tensors. While executing the update operation on the GPU brings significant speedup over its CPU counterpart, it remains a significant bottleneck. To further accelerate the update operation, we propose cuADMM, a new update algorithm that leverages algorithmic and code optimization strategies to minimize both computation and data movement on GPUs. As a result, our framework delivers significantly improved performance compared to prior state-of-the-art. On 10 real-world sparse tensors, our framework achieves geometric mean speedup of 5.1 \texttimes{} (max 41.59 \texttimes{}) and 7.01 \texttimes{} (max 58.05 \texttimes{}) on the NIVIDA A100 and H100 GPUs, respectively, over the state-of-the-art SPLATT library running on a 26-core Intel Ice Lake Xeon CPU.},
booktitle = {Proceedings of the 53rd International Conference on Parallel Processing},
pages = {107–116},
numpages = {10},
keywords = {accelerated sparse tensor Factorization, algorithm, constrained tensor factorization, high performance},
location = {Gotland, Sweden},
series = {ICPP '24}
}

@article{Ahmed2021,
  author       = {Ahmed E. Helal and
                  Jan Laukemann and
                  Fabio Checconi and
                  Jesmin Jahan Tithi and
                  Teresa M. Ranadive and
                  Fabrizio Petrini and
                  Jeewhan Choi},
  title        = {{ALTO:} Adaptive Linearized Storage of Sparse Tensors},
  journal      = {CoRR},
  volume       = {abs/2102.10245},
  year         = {2021},
  url          = {https://arxiv.org/abs/2102.10245},
  eprinttype    = {arXiv},
  eprint       = {2102.10245},
  timestamp    = {Wed, 24 Feb 2021 15:42:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2102-10245.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Blanco2018,
author = {Blanco, Zachary and Liu, Bangtian and Dehnavi, Maryam Mehri},
title = {CSTF: Large-Scale Sparse Tensor Factorizations on Distributed Platforms},
year = {2018},
isbn = {9781450365109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3225058.3225133},
doi = {10.1145/3225058.3225133},
abstract = {Tensors, or N-dimensional arrays, are increasingly used to represent multi-dimensional data. Sparse tensor decomposition algorithms are of particular interest in analyzing and compressing big datasets due to the fact that most of real-world data is sparse and multi-dimensional. However, state-of-the-art tensor decomposition algorithms are not scalable for overwhelmingly large and higher-order sparse tensors on distributed platforms. In this paper, we use the MapReduce model and the Spark engine to implement tensor factorizations on distributed platforms. The proposed CSTF algorithm, Cloud-based Sparse Tensor Factorization, is a scalable distributed algorithm for tensor decompositions for large data. It uses the coordinate storage format (COO) to operate on the tensor nonzeros directly, thus, eliminating the need for tensor unfolding and the storage of intermediate data. Also, a novel queuing strategy (QCOO) is proposed to exploit the dependency and data reuse between a sequence of tensor operations in tensor decomposition algorithms. Details on the key-value storage paradigm and Spark features used to implement the algorithm and the data reuse strategies are also provided. The queuing strategy reduces data communication costs by 35\% for 3rd-order tensors and by 31\% for 4th-order tensors over the COO-based implementation respectively. Compared to the state-of-the-art work, BIGtensor, CSTF achieves 2.2\texttimes{} to 6.9\texttimes{} speedup for 3rd-order tensor decompositions.},
booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
articleno = {21},
numpages = {10},
location = {Eugene, OR, USA},
series = {ICPP '18}
}

@inproceedings{Bharadwaj2024, series={SPAA ’24},
   title={Distributed-Memory Randomized Algorithms for Sparse Tensor CP Decomposition},
   url={http://dx.doi.org/10.1145/3626183.3659980},
   DOI={10.1145/3626183.3659980},
   booktitle={Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures},
   publisher={ACM},
   author={Bharadwaj, Vivek and Malik, Osman Asif and Murray, Riley and Buluç, Aydın and Demmel, James},
   year={2024},
   month=jun, pages={155–168},
   collection={SPAA ’24} }

@INPROCEEDINGS{Nisa2019,
  author={Nisa, Israt and Li, Jiajia and Sukumaran-Rajam, Aravind and Vuduc, Richard and Sadayappan, P.},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Load-Balanced Sparse MTTKRP on GPUs}, 
  year={2019},
  volume={},
  number={},
  pages={123-133},
  keywords={Tensors;Sparse matrices;Matrix decomposition;Indexes;Graphics processing units;Kernel;Multicore processing;MTTKRP;CPD;GPU;Sparse tensor},
  doi={10.1109/IPDPS.2019.00023}}

@inproceedings{Nguyen2022,
author = {Nguyen, Andy and Helal, Ahmed E. and Checconi, Fabio and Laukemann, Jan and Tithi, Jesmin Jahan and Soh, Yongseok and Ranadive, Teresa and Petrini, Fabrizio and Choi, Jee W.},
title = {Efficient, out-of-memory sparse MTTKRP on massively parallel architectures},
year = {2022},
isbn = {9781450392815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524059.3532363},
doi = {10.1145/3524059.3532363},
abstract = {Tensor decomposition (TD) is an important method for extracting latent information from high-dimensional (multi-modal) sparse data. This study presents a novel framework for accelerating fundamental TD operations on massively parallel GPU architectures. In contrast to prior work, the proposed Blocked Linearized Coordinate (BLCO) format enables efficient out-of-memory computation of tensor algorithms using a unified implementation that works on a single tensor copy. Our adaptive blocking and linearization strategies not only meet the resource constraints of GPU devices, but also accelerate data indexing, eliminate control-flow and memory-access irregularities, and reduce kernel launching overhead. To address the substantial synchronization cost on GPUs, we introduce an opportunistic conflict resolution algorithm, in which threads collaborate instead of contending on memory access to discover and resolve their conflicting updates on-the-fly, without keeping any auxiliary information or storing non-zero elements in specific mode orientations. As a result, our framework delivers superior in-memory performance compared to prior state-of-the-art, and is the only framework capable of processing out-of-memory tensors. On the latest Intel and NVIDIA GPUs, BLCO achieves 2.12 -- 2.6X geometric-mean speedup (with up to 33.35X speedup) over the state-of-the-art mixed-mode compressed sparse fiber (MM-CSF) on a range of real-world sparse tensors.},
booktitle = {Proceedings of the 36th ACM International Conference on Supercomputing},
articleno = {26},
numpages = {13},
keywords = {GPU, MTTKRP, parallel performance, sparse formats, sparse tensors, tensor decomposition},
location = {Virtual Event},
series = {ICS '22}
}

@INPROCEEDINGS{Hoefler2009,
  author={Hoefler, Torsten and Traff, Jesper Larsson},
  booktitle={2009 IEEE International Symposium on Parallel & Distributed Processing}, 
  title={Sparse collective operations for MPI}, 
  year={2009},
  volume={},
  number={},
  pages={1-8},
  keywords={National electric code;Europe;Message passing;Libraries;Open systems;Laboratories;Nearest neighbor searches;Communication standards;Error correction;Network topology},
  doi={10.1109/IPDPS.2009.5160935}}


@inproceedings{Fei2021,
author = {Fei, Jiawei and Ho, Chen-Yu and Sahu, Atal N. and Canini, Marco and Sapio, Amedeo},
title = {Efficient sparse collective communication and its application to accelerate distributed deep learning},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472904},
doi = {10.1145/3452296.3472904},
abstract = {Efficient collective communication is crucial to parallel-computing applications such as distributed training of large-scale recommendation systems and natural language processing models. Existing collective communication libraries focus on optimizing operations for dense inputs, resulting in transmissions of many zeros when inputs are sparse. This counters current trends that see increasing data sparsity in large models.We propose OmniReduce, an efficient streaming aggregation system that exploits sparsity to maximize effective bandwidth use by sending only non-zero data blocks. We demonstrate that this idea is beneficial and accelerates distributed training by up to 8.2x. Even at 100 Gbps, OmniReduce delivers 1.4--2.9x better performance for network-bottlenecked DNNs.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {676–691},
numpages = {16},
keywords = {deep learning, distributed training},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@manual{mpi41,
    author = "{Message Passing Interface Forum}",
    title  = "{MPI}: A Message-Passing Interface Standard Version 4.1",
    url    = "https://www.mpi-forum.org/docs/mpi-4.1/mpi41-report.pdf",
    year   = 2023,
    month  = nov
}

@misc{DFacTo,
      title={DFacTo: Distributed Factorization of Tensors},
      author={Joon Hee Choi and S. V. N. Vishwanathan},
      year={2014},
      eprint={1406.4519},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1406.4519},
}

@INPROCEEDINGS{SALS,
  author={Shin, Kijung and Kang, U.},
  booktitle={2014 IEEE International Conference on Data Mining},
  title={Distributed Methods for High-Dimensional and Large-Scale Tensor Factorization},
  year={2014},
  volume={},
  number={},
  pages={989-994},
  keywords={Tensile stress;Scalability;Matrix decomposition;Memory management;Optimization;Distributed databases;Tin;Tensor factorization;Recommender system;Distributed computing;MapReduce},
  doi={10.1109/ICDM.2014.78}}

@inproceedings{Renggli2019,
author = {Renggli, Cedric and Ashkboos, Saleh and Aghagolzadeh, Mehdi and Alistarh, Dan and Hoefler, Torsten},
title = {SparCML: high-performance sparse communication for machine learning},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356222},
doi = {10.1145/3295500.3356222},
abstract = {Applying machine learning techniques to the quickly growing data in science and industry requires highly-scalable algorithms. Large datasets are most commonly processed "data parallel" distributed across many nodes. Each node's contribution to the overall gradient is summed using a global allreduce. This allreduce is the single communication and thus scalability bottleneck for most machine learning workloads. We observe that frequently, many gradient values are (close to) zero, leading to sparse of sparsifyable communications. To exploit this insight, we analyze, design, and implement a set of communication-efficient protocols for sparse input data, in conjunction with efficient machine learning algorithms which can leverage these primitives. Our communication protocols generalize standard collective operations, by allowing processes to contribute arbitrary sparse input data vectors. Our generic communication library, SparCML1, extends MPI to support additional features, such as non-blocking (asynchronous) operations and low-precision data representations. As such, SparCML and its techniques will form the basis of future highly-scalable machine learning frameworks.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {11},
numpages = {15},
keywords = {sparse input vectors, sparse allreduce, sparse allgather},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{Li2022,
author = {Li, Shigang and Hoefler, Torsten},
title = {Near-optimal sparse allreduce for distributed deep learning},
year = {2022},
isbn = {9781450392044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503221.3508399},
doi = {10.1145/3503221.3508399},
abstract = {Communication overhead is one of the major obstacles to train large deep learning models at scale. Gradient sparsification is a promising technique to reduce the communication volume. However, it is very challenging to obtain real performance improvement because of (1) the difficulty of achieving an scalable and efficient sparse allreduce algorithm and (2) the sparsification overhead. This paper proposes Ok-Topk, a scheme for distributed training with sparse gradients. Ok-Topk integrates a novel sparse allreduce algorithm (less than 6k communication volume which is asymptotically optimal) with the decentralized parallel Stochastic Gradient Descent (SGD) optimizer, and its convergence is proved. To reduce the sparsification overhead, Ok-Topk efficiently selects the top-k gradient values according to an estimated threshold. Evaluations are conducted on the Piz Daint supercomputer with neural network models from different deep learning domains. Empirical results show that Ok-Topk achieves similar model accuracy to dense allreduce. Compared with the optimized dense and the state-of-the-art sparse allreduces, Ok-Topk is more scalable and significantly improves training throughput (e.g., 3.29x-12.95x improvement for BERT on 256 GPUs).},
booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {135–149},
numpages = {15},
keywords = {allreduce, data parallelism, distributed deep learning, gradient sparsification},
location = {Seoul, Republic of Korea},
series = {PPoPP '22}
}

@InProceedings{ActorModelPaul2022,
author="Paul, Sri Raj
and Hayashi, Akihiro
and Chen, Kun
and Sarkar, Vivek",
editor="Groen, Derek
and de Mulatier, Cl{\'e}lia
and Paszynski, Maciej
and Krzhizhanovskaya, Valeria V.
and Dongarra, Jack J.
and Sloot, Peter M. A.",
title="A Productive and Scalable Actor-Based Programming System for PGAS Applications",
booktitle="Computational Science -- ICCS 2022",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="233--247",
abstract="The Partitioned Global Address Space (PGAS) model is well suited for executing irregular applications on cluster-based systems, due to its efficient support for short, one-sided messages. Separately, the actor model has been gaining popularity as a productive asynchronous message-passing approach for distributed objects in enterprise and cloud computing platforms, typically implemented in languages such as Erlang, Scala or Rust. To the best of our knowledge, there has been no past work on using the actor model to deliver both productivity and scalability to PGAS applications on clusters.",
isbn="978-3-031-08751-6"
}

@INPROCEEDINGS{Kaya2015,
  author={Kaya, Oguz and Uçar, Bora},
  booktitle={SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={Scalable sparse tensor decompositions in distributed memory systems}, 
  year={2015},
  volume={},
  number={},
  pages={1-11},
  keywords={Tensile stress;Algorithm design and analysis;Signal processing algorithms;Computed tomography;Sparse matrices;Libraries;Approximation algorithms},
  doi={10.1145/2807591.2807624}}

@online{frosttdataset,
  title = {{FROSTT}: The Formidable Repository of Open Sparse Tensors and Tools},
  author = {Smith, Shaden and Choi, Jee W. and Li, Jiajia and Vuduc, Richard and Park, Jongsoo and Liu, Xing and Karypis, George},
  url = {http://frostt.io/},
  year = {2017},
}

@misc{Khalilov2024,
      title={Network-Offloaded Bandwidth-Optimal Broadcast and Allgather for Distributed AI},
      author={Mikhail Khalilov and Salvatore Di Girolamo and Marcin Chrapek and Rami Nudelman and Gil Bloch and Torsten Hoefler},
      year={2024},
      eprint={2408.13356},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2408.13356},
}

@misc{Naumov2024,
      title={Deep Learning Training in Facebook Data Centers: Design of Scale-up and Scale-out Systems},
      author={Maxim Naumov and John Kim and Dheevatsa Mudigere and Srinivas Sridharan and Xiaodong Wang and Whitney Zhao and Serhat Yilmaz and Changkyu Kim and Hector Yuen and Mustafa Ozdal and Krishnakumar Nair and Isabel Gao and Bor-Yiing Su and Jiyan Yang and Mikhail Smelyanskiy},
      year={2020},
      eprint={2003.09518},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2003.09518},
}

@misc{Hong2024,
      title={A sparsity-aware distributed-memory algorithm for sparse-sparse matrix multiplication},
      author={Yuxi Hong and Aydin Buluc},
      year={2024},
      eprint={2408.14558},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2408.14558},
}

@INPROCEEDINGS{KMerNisa2021,
  author={Nisa, Israt and Pandey, Prashant and Ellis, Marquita and Oliker, Leonid and Buluç, Aydın and Yelick, Katherine},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  title={Distributed-Memory k-mer Counting on GPUs},
  year={2021},
  volume={},
  number={},
  pages={527-536},
  keywords={Proteins;Scalability;Graphics processing units;Genomics;Tools;Supercomputers;Partitioning algorithms;GPU;k-mer counter;distributed memory},
  doi={10.1109/IPDPS49936.2021.00061}}

@article{AMGBell2012,
    author = {Bell, Nathan and Dalton, Steven and Olson, Luke N.},
    title = {Exposing Fine-Grained Parallelism in Algebraic Multigrid Methods},
    journal = {SIAM Journal on Scientific Computing},
    volume = {34},
    number = {4},
    pages = {C123-C152},
    year = {2012},
    doi = {10.1137/110838844},
    URL = {https://doi.org/10.1137/110838844},
    eprint = {https://doi.org/10.1137/110838844},
    abstract = { Algebraic multigrid methods for large, sparse linear systems are a necessity in many computational simulations, yet parallel algorithms for such solvers are generally decomposed into coarse-grained tasks suitable for distributed computers with traditional processing cores. However, accelerating multigrid methods on massively parallel throughput-oriented processors, such as graphics processing units, demands algorithms with abundant fine-grained parallelism. In this paper, we develop a parallel algebraic multigrid method which exposes substantial fine-grained parallelism in both the construction of the multigrid hierarchy as well as the cycling or solve stage. Our algorithms are expressed in terms of scalable parallel primitives that are efficiently implemented on the GPU. The resulting solver achieves an average speedup of \$1.8\times\$ in the setup phase and \$5.7\times\$ in the cycling phase when compared to a representative CPU implementation. }
}

@misc{DataCenterHoefler2023,
      title={Datacenter Ethernet and RDMA: Issues at Hyperscale},
      author={Torsten Hoefler and Duncan Roweth and Keith Underwood and Bob Alverson and Mark Griswold and Vahid Tabatabaee and Mohan Kalkunte and Surendra Anubolu and Siyuan Shen and Abdul Kabbani and Moray McLaren and Steve Scott},
      year={2023},
      eprint={2302.03337},
      archivePrefix={arXiv},
      primaryClass={cs.NI},
      url={https://arxiv.org/abs/2302.03337},
}

@inproceedings{RustParrish2023,
author = {Parrish, John and Wren, Nicole and Kiang, Tsz Hang and Hayashi, Akihiro and Young, Jeffrey and Sarkar, Vivek},
title = {Towards Safe HPC: Productivity and Performance via Rust Interfaces for a Distributed C++ Actors Library (Work in Progress)},
year = {2023},
isbn = {9798400703805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617651.3622992},
doi = {10.1145/3617651.3622992},
abstract = {In this work-in-progress research paper, we make the case for using Rust to develop applications in the High Performance Computing (HPC) domain which is critically dependent on native C/C++ libraries. This work explores one example of Safe HPC via the design of a Rust interface to an existing distributed C++ Actors library. This existing library has been shown to deliver high performance to C++ developers of irregular Partitioned Global Address Space (PGAS) applications.

Our key contribution is a proof-of-concept framework to express parallel programs safe-ly in Rust (and potentially other languages/systems), along with a corresponding study of the problems solved by our runtime, the implementation challenges faced, and user productivity. We also conducted an early evaluation of our approach by converting C++ actor implementations of four applications taken from the Bale kernels to Rust Actors using our framework. Our results show that the productivity benefits of our approach are significant since our Rust-based approach helped catch bugs statically during application development, without degrading performance relative to the original C++ actor versions.},
booktitle = {Proceedings of the 20th ACM SIGPLAN International Conference on Managed Programming Languages and Runtimes},
pages = {165–172},
numpages = {8},
keywords = {unsafe annotations, high performance computing, actors, Rust},
location = {Cascais, Portugal},
series = {MPLR 2023}
}

@InProceedings{BlueFieldBayatpour2021,
author="Bayatpour, Mohammadreza
and Sarkauskas, Nick
and Subramoni, Hari
and Maqbool Hashmi, Jahanzeb
and Panda, Dhabaleswar K.",
editor="Chamberlain, Bradford L.
and Varbanescu, Ana-Lucia
and Ltaief, Hatem
and Luszczek, Piotr",
title="BluesMPI: Efficient MPI Non-blocking Alltoall Offloading Designs on Modern BlueField Smart NICs",
booktitle="High Performance Computing",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="18--37",
abstract="In the state-of-the-art production quality MPI (Message Passing Interface) libraries, communication progress is either performed by the main thread or a separate communication progress thread. Taking advantage of separate communication threads can lead to a higher overlap of communication and computation as well as reduced total application execution time. However, such an approach can also lead to contention for CPU resources leading to sub-par application performance as the application itself has less number of available cores for computation. Recently, Mellanox has introduced the BlueField series of adapters which combine the advanced capabilities of traditional ASIC based network adapters with an array of ARM processors. In this paper, we propose BluesMPI, a high performance MPI non-blocking Alltoall design that can be used to offload MPI{\_}Ialltoall collective operations from the host CPU to the Smart NIC. BluesMPI guarantees the full overlap of communication and computation for Alltoall collective operations while providing on-par pure communication latency to CPU based on-loading designs. We explore several designs to achieve the best pure communication latency for MPI{\_}Ialltoall. Our experiments show that BluesMPI can improve the total execution time of the OSU Micro Benchmark for MPI{\_}Ialltoall and P3DFFT application up to 44{\%} and 30{\%}, respectively. To the best of our knowledge, this is the first design that efficiently takes advantage of modern BlueField Smart NICs in deriving the MPI Alltoall collective operation to get peak overlap of communication and computation.",
isbn="978-3-030-78713-4"
}

@techreport{Karamati2021,
  author       = {Karamati, Sara and Young, Jeffrey and Conte, Tom and Hemmert, Karl S. and Grant, Ryan and Hughes, Clayton and Vuduc, Rich},
  title        = {Computational Offload with BlueField Smart NICs},
  institution  = {Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)},
  annote       = {The recent introduction of a new generation of "smart NICs" have provided new accelerator platforms that include CPU cores or reconfigurable fabric in addition to traditional networking hardware and packet offloading capabilities. While there are currently several proposals for using these smartNICs for low-latency, in-line packet processing operations, there remains a gap in knowledge as to how they might be used as computational accelerators for traditional high-performance applications. This work aims to look at benchmarks and mini-applications to evaluate possible benefits of using a smartNIC as a compute accelerator for HPC applications. We investigate NVIDIA's current-generation BlueField-2 card, which includes eight Arm CPUs along with a small amount of storage, and we test the networking and data movement performance of these cards compared to a standard Intel server host. We then detail how two different applications, YASK and miniMD can be modified to make more efficient use of the BlueField-2 device with a focus on overlapping computation and communication for operations like neighbor building and halo exchanges. Our results show that while the overall compute performance of these devices is limited, using them with a modified miniMD algorithm allows for potential speedups of 5 to 20\% over the host CPU baseline with no loss in simulation accuracy.},
  doi          = {10.2172/1832297},
  url          = {https://www.osti.gov/biblio/1832297},
  place        = {United States},
  year         = {2021},
  month        = {10}}

@article{RooflineModel,
author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
title = {Roofline: an insightful visual performance model for multicore architectures},
year = {2009},
issue_date = {April 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/1498765.1498785},
doi = {10.1145/1498765.1498785},
abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
journal = {Commun. ACM},
month = apr,
pages = {65–76},
numpages = {12}
}

@article{genten-2019,
author = {Phipps, Eric T. and Kolda, Tamara G.},
title = {Software for Sparse Tensor Decomposition on Emerging Computing Architectures},
journal = {SIAM Journal on Scientific Computing},
volume = {41},
number = {3},
pages = {C269-C290},
year = {2019},
doi = {10.1137/18M1210691},
URL = { https://doi.org/10.1137/18M1210691},
eprint = {https://doi.org/10.1137/18M1210691},
    abstract = { In this paper, we develop software for decomposing sparse tensors that is portable to and performant on a variety of multicore, manycore, and GPU computing architectures. The result is a single code whose performance matches optimized architecture-specific implementations. The key to a portable approach is to determine multiple levels of parallelism that can be mapped in different ways to different architectures, and we explain how to do this for the matricized tensor times Khatri--Rao product (MTTKRP), which is the key kernel in canonical polyadic tensor decomposition. Our implementation leverages the Kokkos framework, which enables a single code to achieve high performance across multiple architectures that differ in how they approach fine-grained parallelism. We also introduce a new construct for portable thread-local arrays, which we call compile-time polymorphic arrays. Not only are the specifics of our approaches and implementation interesting for tuning tensor computations, but they also provide a roadmap for developing other portable high-performance codes. As a last step in optimizing performance, we modify the MTTKRP algorithm itself to do a permuted traversal of tensor nonzeros to reduce atomic-write contention. We test the performance of our implementation on 16- and 68-core Intel CPUs and the K80 and P100 NVIDIA GPUs, showing that we are competitive with state-of-the-art architecture-specific codes while having the advantage of being able to run on a variety of architectures. }
}

@INPROCEEDINGS{genten-2021,
  author={Lewis, Cannada and Phipps, Eric},
  booktitle={2021 IEEE High Performance Extreme Computing Conference (HPEC)},
  title={Low-Communication Asynchronous Distributed Generalized Canonical Polyadic Tensor Decomposition},
  year={2021},
  volume={},
  number={},
  pages={1-5},
  keywords={Tensors;Costs;Conferences;Convergence;Tensor Decomposition;Canonical Polyadic;Stochastic Gradient Descent;Asynchrony;Federated Learning},
  doi={10.1109/HPEC49654.2021.9622844}}

@inproceedings{genten-2023,
author = {Phipps, Eric T. and Johnson, Nicholas T. and Kolda, Tamara G.},
title = {Streaming Generalized Canonical Polyadic Tensor Decompositions},
year = {2023},
isbn = {9798400701900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3592979.3593405},
doi = {10.1145/3592979.3593405},
abstract = {In this paper, we develop a method which we call OnlineGCP for computing the Generalized Canonical Polyadic (GCP) tensor decomposition of streaming data. GCP differs from traditional canonical polyadic (CP) tensor decompositions as it allows for arbitrary objective functions which the CP model attempts to minimize. This approach can provide better fits and more interpretable models when the observed tensor data is strongly non-Gaussian. In the streaming case, tensor data is gradually observed over time and the algorithm must incrementally update a GCP factorization with limited access to prior data. In this work, we extend the GCP formalism to the streaming context by deriving a GCP optimization problem to be solved as new tensor data is observed, formulate a tunable history term to balance reconstruction of recently observed data with data observed in the past, develop a scalable solution strategy based on segregated solves using stochastic gradient descent methods, describe a software implementation that provides performance and portability to contemporary CPU and GPU architectures and demonstrate the utility and performance of the approach and software on several synthetic and real tensor data sets.},
booktitle = {Proceedings of the Platform for Advanced Scientific Computing Conference},
articleno = {6},
numpages = {10},
keywords = {streaming, canonical polyadic, tensor decomposition},
location = {Davos, Switzerland},
series = {PASC '23}
}

@inproceedings{sanders-2023,
   title={Engineering a Distributed-Memory Triangle Counting Algorithm},
   url={http://dx.doi.org/10.1109/IPDPS54959.2023.00076},
   DOI={10.1109/ipdps54959.2023.00076},
   booktitle={2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
   publisher={IEEE},
   author={Sanders, Peter and Uhl, Tim Niklas},
   year={2023},
   month=may, pages={702–712}}

@article{miao-2023,
author = {Miao, Zheng and Calhoun, Jon C. and Ge, Rong and Li, Jiajia},
title = {Performance Implication of Tensor Irregularity and Optimization for Distributed Tensor Decomposition},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {2329-4949},
url = {https://doi.org/10.1145/3580315},
doi = {10.1145/3580315},
abstract = {Tensors are used by a wide variety of applications to represent multi-dimensional data; tensor decompositions are a class of methods for latent data analytics, data compression, and so on. Many of these applications generate large tensors with irregular dimension sizes and nonzero distribution. CANDECOMP/PARAFAC decomposition (Cpd) is a popular low-rank tensor decomposition for discovering latent features. The increasing overhead on memory and execution time of Cpd for large tensors requires distributed memory implementations as the only feasible solution. The sparsity and irregularity of tensors hinder the improvement of performance and scalability of distributed memory implementations. While previous works have been proved successful in Cpd for tensors with relatively regular dimension sizes and nonzero distribution, they either deliver unsatisfactory performance and scalability for irregular tensors or require significant time overhead in preprocessing. In this work, we focus on medium-grained tensor distribution to address their limitation for irregular tensors. We first thoroughly investigate through theoretical and experimental analysis. We disclose that the main cause of poor Cpd performance and scalability is the imbalance of multiple types of computations and communications and their tradeoffs; and sparsity and irregularity make it challenging to achieve their balances and tradeoffs. Irregularity of a sparse tensor is categorized based on two aspects: very different dimension sizes and a non-uniform nonzero distribution. Typically, focusing on optimizing one type of load imbalance causes other ones more severe for irregular tensors. To address such challenges, we propose irregularity-aware distributed Cpd that leverages the sparsity and irregularity information to identify the best tradeoff between different imbalances with low time overhead. We materialize the idea with two optimization methods: the prediction-based grid configuration and matrix-oriented distribution policy, where the former forms the global balance among computations and communications, and the latter further adjusts the balances among computations. The experimental results show that our proposed irregularity-aware distributed Cpd is more scalable and outperforms the medium- and fine-grained distributed implementations by up to 4.4 \texttimes{} and 11.4 \texttimes{} on 1,536 processors, respectively. Our optimizations support different sparse tensor formats, such as compressed sparse fiber (CSF), coordinate (COO), and Hierarchical Coordinate (HiCOO), and gain good scalability for all of them.},
journal = {ACM Trans. Parallel Comput.},
month = jun,
articleno = {10},
numpages = {27},
keywords = {Sparse tensor, tensor decomposition, CPD, irregularity}
}
