@INPROCEEDINGS{Smith2016,
  author={Smith, Shaden and Karypis, George},
  booktitle={2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={A Medium-Grained Algorithm for Sparse Tensor Factorization}, 
  year={2016},
  volume={},
  number={},
  pages={902-911},
  keywords={Tensile stress;Matrix decomposition;Sparse matrices;Distributed databases;Memory management;Parallel processing;Scalability;Sparse tensor;distributed;PARAFAC;CPD;parallel;medium-grained},
  doi={10.1109/IPDPS.2016.113}}

@INPROCEEDINGS{Smith2015,
  author={Smith, Shaden and Ravindran, Niranjay and Sidiropoulos, Nicholas D. and Karypis, George},
  booktitle={2015 IEEE International Parallel and Distributed Processing Symposium},
  title={SPLATT: Efficient and Parallel Sparse Tensor-Matrix Multiplication},
  year={2015},
  volume={},
  number={},
  pages={61-70},
  keywords={Tensile stress;Sparse matrices;Data structures;Memory management;Context;Algorithm design and analysis;Parallel processing;Sparse tensors;PARAFAC;CANDECOMP;CPD;parallel},
  doi={10.1109/IPDPS.2015.27}}

@inproceedings{Soh2024,
author = {Soh, Yongseok and Kannan, Ramakrishnan and Sao, Piyush and Choi, Jee},
title = {Accelerated Constrained Sparse Tensor Factorization on Massively Parallel Architectures},
year = {2024},
isbn = {9798400717932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3673038.3673128},
doi = {10.1145/3673038.3673128},
abstract = {This study presents the first constrained sparse tensor factorization (cSTF) framework that optimizes and fully offloads computation to massively parallel GPU architectures, and the first performance characterization of cSTF on GPU architectures. In contrast to prior work on tensor factorization, where the matricized tensor times Khatri-Rao product (MTTKRP) is the primary performance bottleneck, our systematic analysis of the cSTF algorithm on GPUs reveals that adding constraints creates an additional bottleneck in the update operation for many real-world sparse tensors. While executing the update operation on the GPU brings significant speedup over its CPU counterpart, it remains a significant bottleneck. To further accelerate the update operation, we propose cuADMM, a new update algorithm that leverages algorithmic and code optimization strategies to minimize both computation and data movement on GPUs. As a result, our framework delivers significantly improved performance compared to prior state-of-the-art. On 10 real-world sparse tensors, our framework achieves geometric mean speedup of 5.1 \texttimes{} (max 41.59 \texttimes{}) and 7.01 \texttimes{} (max 58.05 \texttimes{}) on the NIVIDA A100 and H100 GPUs, respectively, over the state-of-the-art SPLATT library running on a 26-core Intel Ice Lake Xeon CPU.},
booktitle = {Proceedings of the 53rd International Conference on Parallel Processing},
pages = {107–116},
numpages = {10},
keywords = {accelerated sparse tensor Factorization, algorithm, constrained tensor factorization, high performance},
location = {Gotland, Sweden},
series = {ICPP '24}
}

@article{Ahmed2021,
  author       = {Ahmed E. Helal and
                  Jan Laukemann and
                  Fabio Checconi and
                  Jesmin Jahan Tithi and
                  Teresa M. Ranadive and
                  Fabrizio Petrini and
                  Jeewhan Choi},
  title        = {{ALTO:} Adaptive Linearized Storage of Sparse Tensors},
  journal      = {CoRR},
  volume       = {abs/2102.10245},
  year         = {2021},
  url          = {https://arxiv.org/abs/2102.10245},
  eprinttype    = {arXiv},
  eprint       = {2102.10245},
  timestamp    = {Wed, 24 Feb 2021 15:42:45 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2102-10245.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Blanco2018,
author = {Blanco, Zachary and Liu, Bangtian and Dehnavi, Maryam Mehri},
title = {CSTF: Large-Scale Sparse Tensor Factorizations on Distributed Platforms},
year = {2018},
isbn = {9781450365109},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3225058.3225133},
doi = {10.1145/3225058.3225133},
abstract = {Tensors, or N-dimensional arrays, are increasingly used to represent multi-dimensional data. Sparse tensor decomposition algorithms are of particular interest in analyzing and compressing big datasets due to the fact that most of real-world data is sparse and multi-dimensional. However, state-of-the-art tensor decomposition algorithms are not scalable for overwhelmingly large and higher-order sparse tensors on distributed platforms. In this paper, we use the MapReduce model and the Spark engine to implement tensor factorizations on distributed platforms. The proposed CSTF algorithm, Cloud-based Sparse Tensor Factorization, is a scalable distributed algorithm for tensor decompositions for large data. It uses the coordinate storage format (COO) to operate on the tensor nonzeros directly, thus, eliminating the need for tensor unfolding and the storage of intermediate data. Also, a novel queuing strategy (QCOO) is proposed to exploit the dependency and data reuse between a sequence of tensor operations in tensor decomposition algorithms. Details on the key-value storage paradigm and Spark features used to implement the algorithm and the data reuse strategies are also provided. The queuing strategy reduces data communication costs by 35\% for 3rd-order tensors and by 31\% for 4th-order tensors over the COO-based implementation respectively. Compared to the state-of-the-art work, BIGtensor, CSTF achieves 2.2\texttimes{} to 6.9\texttimes{} speedup for 3rd-order tensor decompositions.},
booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
articleno = {21},
numpages = {10},
location = {Eugene, OR, USA},
series = {ICPP '18}
}

@inproceedings{Bharadwaj2024, series={SPAA ’24},
   title={Distributed-Memory Randomized Algorithms for Sparse Tensor CP Decomposition},
   url={http://dx.doi.org/10.1145/3626183.3659980},
   DOI={10.1145/3626183.3659980},
   booktitle={Proceedings of the 36th ACM Symposium on Parallelism in Algorithms and Architectures},
   publisher={ACM},
   author={Bharadwaj, Vivek and Malik, Osman Asif and Murray, Riley and Buluç, Aydın and Demmel, James},
   year={2024},
   month=jun, pages={155–168},
   collection={SPAA ’24} }

@INPROCEEDINGS{Nisa2019,
  author={Nisa, Israt and Li, Jiajia and Sukumaran-Rajam, Aravind and Vuduc, Richard and Sadayappan, P.},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={Load-Balanced Sparse MTTKRP on GPUs}, 
  year={2019},
  volume={},
  number={},
  pages={123-133},
  keywords={Tensors;Sparse matrices;Matrix decomposition;Indexes;Graphics processing units;Kernel;Multicore processing;MTTKRP;CPD;GPU;Sparse tensor},
  doi={10.1109/IPDPS.2019.00023}}

@inproceedings{Nguyen2022,
author = {Nguyen, Andy and Helal, Ahmed E. and Checconi, Fabio and Laukemann, Jan and Tithi, Jesmin Jahan and Soh, Yongseok and Ranadive, Teresa and Petrini, Fabrizio and Choi, Jee W.},
title = {Efficient, out-of-memory sparse MTTKRP on massively parallel architectures},
year = {2022},
isbn = {9781450392815},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524059.3532363},
doi = {10.1145/3524059.3532363},
abstract = {Tensor decomposition (TD) is an important method for extracting latent information from high-dimensional (multi-modal) sparse data. This study presents a novel framework for accelerating fundamental TD operations on massively parallel GPU architectures. In contrast to prior work, the proposed Blocked Linearized Coordinate (BLCO) format enables efficient out-of-memory computation of tensor algorithms using a unified implementation that works on a single tensor copy. Our adaptive blocking and linearization strategies not only meet the resource constraints of GPU devices, but also accelerate data indexing, eliminate control-flow and memory-access irregularities, and reduce kernel launching overhead. To address the substantial synchronization cost on GPUs, we introduce an opportunistic conflict resolution algorithm, in which threads collaborate instead of contending on memory access to discover and resolve their conflicting updates on-the-fly, without keeping any auxiliary information or storing non-zero elements in specific mode orientations. As a result, our framework delivers superior in-memory performance compared to prior state-of-the-art, and is the only framework capable of processing out-of-memory tensors. On the latest Intel and NVIDIA GPUs, BLCO achieves 2.12 -- 2.6X geometric-mean speedup (with up to 33.35X speedup) over the state-of-the-art mixed-mode compressed sparse fiber (MM-CSF) on a range of real-world sparse tensors.},
booktitle = {Proceedings of the 36th ACM International Conference on Supercomputing},
articleno = {26},
numpages = {13},
keywords = {GPU, MTTKRP, parallel performance, sparse formats, sparse tensors, tensor decomposition},
location = {Virtual Event},
series = {ICS '22}
}

@INPROCEEDINGS{Hoefler2009,
  author={Hoefler, Torsten and Traff, Jesper Larsson},
  booktitle={2009 IEEE International Symposium on Parallel & Distributed Processing}, 
  title={Sparse collective operations for MPI}, 
  year={2009},
  volume={},
  number={},
  pages={1-8},
  keywords={National electric code;Europe;Message passing;Libraries;Open systems;Laboratories;Nearest neighbor searches;Communication standards;Error correction;Network topology},
  doi={10.1109/IPDPS.2009.5160935}}


@inproceedings{Fei2021,
author = {Fei, Jiawei and Ho, Chen-Yu and Sahu, Atal N. and Canini, Marco and Sapio, Amedeo},
title = {Efficient sparse collective communication and its application to accelerate distributed deep learning},
year = {2021},
isbn = {9781450383837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3452296.3472904},
doi = {10.1145/3452296.3472904},
abstract = {Efficient collective communication is crucial to parallel-computing applications such as distributed training of large-scale recommendation systems and natural language processing models. Existing collective communication libraries focus on optimizing operations for dense inputs, resulting in transmissions of many zeros when inputs are sparse. This counters current trends that see increasing data sparsity in large models.We propose OmniReduce, an efficient streaming aggregation system that exploits sparsity to maximize effective bandwidth use by sending only non-zero data blocks. We demonstrate that this idea is beneficial and accelerates distributed training by up to 8.2x. Even at 100 Gbps, OmniReduce delivers 1.4--2.9x better performance for network-bottlenecked DNNs.},
booktitle = {Proceedings of the 2021 ACM SIGCOMM 2021 Conference},
pages = {676–691},
numpages = {16},
keywords = {deep learning, distributed training},
location = {Virtual Event, USA},
series = {SIGCOMM '21}
}

@manual{mpi41,
    author = "{Message Passing Interface Forum}",
    title  = "{MPI}: A Message-Passing Interface Standard Version 4.1",
    url    = "https://www.mpi-forum.org/docs/mpi-4.1/mpi41-report.pdf",
    year   = 2023,
    month  = nov
}

@misc{DFacTo,
      title={DFacTo: Distributed Factorization of Tensors},
      author={Joon Hee Choi and S. V. N. Vishwanathan},
      year={2014},
      eprint={1406.4519},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1406.4519},
}

@INPROCEEDINGS{SALS,
  author={Shin, Kijung and Kang, U.},
  booktitle={2014 IEEE International Conference on Data Mining},
  title={Distributed Methods for High-Dimensional and Large-Scale Tensor Factorization},
  year={2014},
  volume={},
  number={},
  pages={989-994},
  keywords={Tensile stress;Scalability;Matrix decomposition;Memory management;Optimization;Distributed databases;Tin;Tensor factorization;Recommender system;Distributed computing;MapReduce},
  doi={10.1109/ICDM.2014.78}}

@inproceedings{Renggli2019,
author = {Renggli, Cedric and Ashkboos, Saleh and Aghagolzadeh, Mehdi and Alistarh, Dan and Hoefler, Torsten},
title = {SparCML: high-performance sparse communication for machine learning},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3295500.3356222},
doi = {10.1145/3295500.3356222},
abstract = {Applying machine learning techniques to the quickly growing data in science and industry requires highly-scalable algorithms. Large datasets are most commonly processed "data parallel" distributed across many nodes. Each node's contribution to the overall gradient is summed using a global allreduce. This allreduce is the single communication and thus scalability bottleneck for most machine learning workloads. We observe that frequently, many gradient values are (close to) zero, leading to sparse of sparsifyable communications. To exploit this insight, we analyze, design, and implement a set of communication-efficient protocols for sparse input data, in conjunction with efficient machine learning algorithms which can leverage these primitives. Our communication protocols generalize standard collective operations, by allowing processes to contribute arbitrary sparse input data vectors. Our generic communication library, SparCML1, extends MPI to support additional features, such as non-blocking (asynchronous) operations and low-precision data representations. As such, SparCML and its techniques will form the basis of future highly-scalable machine learning frameworks.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {11},
numpages = {15},
keywords = {sparse input vectors, sparse allreduce, sparse allgather},
location = {Denver, Colorado},
series = {SC '19}
}

@inproceedings{Li2022,
author = {Li, Shigang and Hoefler, Torsten},
title = {Near-optimal sparse allreduce for distributed deep learning},
year = {2022},
isbn = {9781450392044},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503221.3508399},
doi = {10.1145/3503221.3508399},
abstract = {Communication overhead is one of the major obstacles to train large deep learning models at scale. Gradient sparsification is a promising technique to reduce the communication volume. However, it is very challenging to obtain real performance improvement because of (1) the difficulty of achieving an scalable and efficient sparse allreduce algorithm and (2) the sparsification overhead. This paper proposes Ok-Topk, a scheme for distributed training with sparse gradients. Ok-Topk integrates a novel sparse allreduce algorithm (less than 6k communication volume which is asymptotically optimal) with the decentralized parallel Stochastic Gradient Descent (SGD) optimizer, and its convergence is proved. To reduce the sparsification overhead, Ok-Topk efficiently selects the top-k gradient values according to an estimated threshold. Evaluations are conducted on the Piz Daint supercomputer with neural network models from different deep learning domains. Empirical results show that Ok-Topk achieves similar model accuracy to dense allreduce. Compared with the optimized dense and the state-of-the-art sparse allreduces, Ok-Topk is more scalable and significantly improves training throughput (e.g., 3.29x-12.95x improvement for BERT on 256 GPUs).},
booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {135–149},
numpages = {15},
keywords = {allreduce, data parallelism, distributed deep learning, gradient sparsification},
location = {Seoul, Republic of Korea},
series = {PPoPP '22}
}

@InProceedings{ActorModelPaul2022,
author="Paul, Sri Raj
and Hayashi, Akihiro
and Chen, Kun
and Sarkar, Vivek",
editor="Groen, Derek
and de Mulatier, Cl{\'e}lia
and Paszynski, Maciej
and Krzhizhanovskaya, Valeria V.
and Dongarra, Jack J.
and Sloot, Peter M. A.",
title="A Productive and Scalable Actor-Based Programming System for PGAS Applications",
booktitle="Computational Science -- ICCS 2022",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="233--247",
abstract="The Partitioned Global Address Space (PGAS) model is well suited for executing irregular applications on cluster-based systems, due to its efficient support for short, one-sided messages. Separately, the actor model has been gaining popularity as a productive asynchronous message-passing approach for distributed objects in enterprise and cloud computing platforms, typically implemented in languages such as Erlang, Scala or Rust. To the best of our knowledge, there has been no past work on using the actor model to deliver both productivity and scalability to PGAS applications on clusters.",
isbn="978-3-031-08751-6"
}

@INPROCEEDINGS{Kaya2015,
  author={Kaya, Oguz and Uçar, Bora},
  booktitle={SC '15: Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis}, 
  title={Scalable sparse tensor decompositions in distributed memory systems}, 
  year={2015},
  volume={},
  number={},
  pages={1-11},
  keywords={Tensile stress;Algorithm design and analysis;Signal processing algorithms;Computed tomography;Sparse matrices;Libraries;Approximation algorithms},
  doi={10.1145/2807591.2807624}}

@online{frosttdataset,
  title = {{FROSTT}: The Formidable Repository of Open Sparse Tensors and Tools},
  author = {Smith, Shaden and Choi, Jee W. and Li, Jiajia and Vuduc, Richard and Park, Jongsoo and Liu, Xing and Karypis, George},
  url = {http://frostt.io/},
  year = {2017},
}

@misc{Khalilov2024,
      title={Network-Offloaded Bandwidth-Optimal Broadcast and Allgather for Distributed AI},
      author={Mikhail Khalilov and Salvatore Di Girolamo and Marcin Chrapek and Rami Nudelman and Gil Bloch and Torsten Hoefler},
      year={2024},
      eprint={2408.13356},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2408.13356},
}

@misc{Naumov2024,
      title={Deep Learning Training in Facebook Data Centers: Design of Scale-up and Scale-out Systems},
      author={Maxim Naumov and John Kim and Dheevatsa Mudigere and Srinivas Sridharan and Xiaodong Wang and Whitney Zhao and Serhat Yilmaz and Changkyu Kim and Hector Yuen and Mustafa Ozdal and Krishnakumar Nair and Isabel Gao and Bor-Yiing Su and Jiyan Yang and Mikhail Smelyanskiy},
      year={2020},
      eprint={2003.09518},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2003.09518},
}

@misc{Hong2024,
      title={A sparsity-aware distributed-memory algorithm for sparse-sparse matrix multiplication},
      author={Yuxi Hong and Aydin Buluc},
      year={2024},
      eprint={2408.14558},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2408.14558},
}

@INPROCEEDINGS{KMerNisa2021,
  author={Nisa, Israt and Pandey, Prashant and Ellis, Marquita and Oliker, Leonid and Buluç, Aydın and Yelick, Katherine},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  title={Distributed-Memory k-mer Counting on GPUs},
  year={2021},
  volume={},
  number={},
  pages={527-536},
  keywords={Proteins;Scalability;Graphics processing units;Genomics;Tools;Supercomputers;Partitioning algorithms;GPU;k-mer counter;distributed memory},
  doi={10.1109/IPDPS49936.2021.00061}}
